{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all of the models you create, choose a threshold that optimizes for accuracy.\n",
    "\n",
    "Create a new notebook, logistic_regression, use it to answer the following questions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_id  survived  pclass     sex   age  sibsp  parch     fare  \\\n",
       "0             0         0       3    male  22.0      1      0   7.2500   \n",
       "1             1         1       1  female  38.0      1      0  71.2833   \n",
       "2             2         1       3  female  26.0      0      0   7.9250   \n",
       "\n",
       "  embarked  class deck  embark_town  alone  \n",
       "0        S  Third  NaN  Southampton      0  \n",
       "1        C  First    C    Cherbourg      0  \n",
       "2        S  Third  NaN  Southampton      1  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = acquire.get_titanic_data()\n",
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((498, 12), (214, 12), (179, 12))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validate, test = prepare.clean_titanic(titanic)\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA INTO X AND Y DATA SETS\n",
    "X_train = train.drop(columns=['survived', 'sex', 'embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'sex_male', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "X_validate = validate.drop(columns=['survived','sex','embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'sex_male', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "X_test = test.drop(columns=['survived','sex','embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'sex_male', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "y_train = train.survived\n",
    "\n",
    "y_validate = validate.survived \n",
    "\n",
    "y_test = test.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass   age     fare\n",
       "271       3  25.0   0.0000\n",
       "786       3  18.0   7.4958\n",
       "86        3  16.0  34.3750\n",
       "353       3  25.0  17.8000\n",
       "199       2  24.0  13.0000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a model that includes only age, fare, and pclass. Does this model perform better than your baseline?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6164658634538153"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = (train.survived == 0).mean()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, random_state=123)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-0.91716114 -0.03482089  0.00649545]]\n",
      "Intercept: \n",
      " [2.40727567]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit.coef_)\n",
    "print('Intercept: \\n', logit.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logit.predict_proba(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.71\n",
      " Baseline Accuracy is 0.6164658634538153\n",
      "The model performed better than the baseline\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train, y_train)))\n",
    "print(f' Baseline Accuracy is {baseline_accuracy}')\n",
    "print('The model performed better than the baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[265  42]\n",
      " [103  88]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.79       307\n",
      "           1       0.68      0.46      0.55       191\n",
      "\n",
      "    accuracy                           0.71       498\n",
      "   macro avg       0.70      0.66      0.67       498\n",
      "weighted avg       0.70      0.71      0.69       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Include sex in your model as well. Note that you'll need to encode or create a dummy variable of this feature before including it in a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = train.drop(columns=['survived', 'sex', 'embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "X_validate2= validate.drop(columns=['survived','sex','embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "X_test2 = test.drop(columns=['survived','sex','embark_town', 'embark_town_Queenstown', 'embark_town_Southampton', 'alone', 'sibsp', 'parch'])\n",
    "\n",
    "y_train2 = train.survived\n",
    "\n",
    "y_validate2 = validate.survived \n",
    "\n",
    "y_test2 = test.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit2 = LogisticRegression(C=1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, random_state=123)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-1.16496558e+00 -3.68064104e-02  1.18200217e-03 -2.40812807e+00]]\n",
      "Intercept: \n",
      " [4.6524267]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit2.coef_)\n",
    "print('Intercept: \\n', logit2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2 = logit2.predict(X_train2)\n",
    "y_pred2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8975927 , 0.1024073 ],\n",
       "       [0.37663055, 0.62336945],\n",
       "       [0.85801125, 0.14198875],\n",
       "       [0.89564249, 0.10435751],\n",
       "       [0.18930879, 0.81069121],\n",
       "       [0.8662231 , 0.1337769 ],\n",
       "       [0.86176001, 0.13823999],\n",
       "       [0.47788258, 0.52211742],\n",
       "       [0.91255795, 0.08744205],\n",
       "       [0.52348753, 0.47651247],\n",
       "       [0.87439661, 0.12560339],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.92618347, 0.07381653],\n",
       "       [0.83000806, 0.16999194],\n",
       "       [0.26686284, 0.73313716],\n",
       "       [0.48298579, 0.51701421],\n",
       "       [0.90652825, 0.09347175],\n",
       "       [0.80143658, 0.19856342],\n",
       "       [0.93395433, 0.06604567],\n",
       "       [0.91118137, 0.08881863],\n",
       "       [0.83716457, 0.16283543],\n",
       "       [0.14496611, 0.85503389],\n",
       "       [0.52046451, 0.47953549],\n",
       "       [0.78788582, 0.21211418],\n",
       "       [0.88971781, 0.11028219],\n",
       "       [0.55303237, 0.44696763],\n",
       "       [0.24201696, 0.75798304],\n",
       "       [0.06283185, 0.93716815],\n",
       "       [0.92099015, 0.07900985],\n",
       "       [0.76345576, 0.23654424],\n",
       "       [0.41169106, 0.58830894],\n",
       "       [0.88229988, 0.11770012],\n",
       "       [0.98138656, 0.01861344],\n",
       "       [0.87108508, 0.12891492],\n",
       "       [0.91546214, 0.08453786],\n",
       "       [0.47986014, 0.52013986],\n",
       "       [0.33456906, 0.66543094],\n",
       "       [0.20293958, 0.79706042],\n",
       "       [0.48815042, 0.51184958],\n",
       "       [0.74982317, 0.25017683],\n",
       "       [0.50958554, 0.49041446],\n",
       "       [0.37655307, 0.62344693],\n",
       "       [0.90010346, 0.09989654],\n",
       "       [0.21968197, 0.78031803],\n",
       "       [0.69951179, 0.30048821],\n",
       "       [0.58118544, 0.41881456],\n",
       "       [0.8968023 , 0.1031977 ],\n",
       "       [0.8468949 , 0.1531051 ],\n",
       "       [0.29749604, 0.70250396],\n",
       "       [0.07079364, 0.92920636],\n",
       "       [0.48691655, 0.51308345],\n",
       "       [0.40284192, 0.59715808],\n",
       "       [0.54809454, 0.45190546],\n",
       "       [0.96812709, 0.03187291],\n",
       "       [0.59698624, 0.40301376],\n",
       "       [0.07851656, 0.92148344],\n",
       "       [0.48916107, 0.51083893],\n",
       "       [0.90009505, 0.09990495],\n",
       "       [0.90503604, 0.09496396],\n",
       "       [0.14584594, 0.85415406],\n",
       "       [0.47298518, 0.52701482],\n",
       "       [0.10735144, 0.89264856],\n",
       "       [0.06477444, 0.93522556],\n",
       "       [0.41184137, 0.58815863],\n",
       "       [0.93563652, 0.06436348],\n",
       "       [0.87522347, 0.12477653],\n",
       "       [0.81613087, 0.18386913],\n",
       "       [0.58813994, 0.41186006],\n",
       "       [0.0847703 , 0.9152297 ],\n",
       "       [0.50049488, 0.49950512],\n",
       "       [0.87848716, 0.12151284],\n",
       "       [0.29589901, 0.70410099],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.87889245, 0.12110755],\n",
       "       [0.8932731 , 0.1067269 ],\n",
       "       [0.76345576, 0.23654424],\n",
       "       [0.50168858, 0.49831142],\n",
       "       [0.83202296, 0.16797704],\n",
       "       [0.36791119, 0.63208881],\n",
       "       [0.77833504, 0.22166496],\n",
       "       [0.15656059, 0.84343941],\n",
       "       [0.89674485, 0.10325515],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.05073239, 0.94926761],\n",
       "       [0.65156161, 0.34843839],\n",
       "       [0.10125314, 0.89874686],\n",
       "       [0.87050548, 0.12949452],\n",
       "       [0.33577847, 0.66422153],\n",
       "       [0.93951594, 0.06048406],\n",
       "       [0.90813191, 0.09186809],\n",
       "       [0.36611717, 0.63388283],\n",
       "       [0.44772993, 0.55227007],\n",
       "       [0.46598181, 0.53401819],\n",
       "       [0.47528912, 0.52471088],\n",
       "       [0.75097162, 0.24902838],\n",
       "       [0.58337538, 0.41662462],\n",
       "       [0.07539016, 0.92460984],\n",
       "       [0.76048392, 0.23951608],\n",
       "       [0.85743524, 0.14256476],\n",
       "       [0.67732722, 0.32267278],\n",
       "       [0.07939766, 0.92060234],\n",
       "       [0.80190641, 0.19809359],\n",
       "       [0.75779114, 0.24220886],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.80482647, 0.19517353],\n",
       "       [0.88612929, 0.11387071],\n",
       "       [0.31174104, 0.68825896],\n",
       "       [0.41169822, 0.58830178],\n",
       "       [0.91112874, 0.08887126],\n",
       "       [0.90335214, 0.09664786],\n",
       "       [0.57915622, 0.42084378],\n",
       "       [0.90651823, 0.09348177],\n",
       "       [0.76432881, 0.23567119],\n",
       "       [0.47555199, 0.52444801],\n",
       "       [0.21037806, 0.78962194],\n",
       "       [0.93344843, 0.06655157],\n",
       "       [0.93332993, 0.06667007],\n",
       "       [0.9117957 , 0.0882043 ],\n",
       "       [0.88228504, 0.11771496],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.09566456, 0.90433544],\n",
       "       [0.79864753, 0.20135247],\n",
       "       [0.85724452, 0.14275548],\n",
       "       [0.214161  , 0.785839  ],\n",
       "       [0.21833184, 0.78166816],\n",
       "       [0.90009063, 0.09990937],\n",
       "       [0.37451229, 0.62548771],\n",
       "       [0.64995916, 0.35004084],\n",
       "       [0.90959302, 0.09040698],\n",
       "       [0.52666439, 0.47333561],\n",
       "       [0.26967714, 0.73032286],\n",
       "       [0.95119384, 0.04880616],\n",
       "       [0.59138885, 0.40861115],\n",
       "       [0.90995494, 0.09004506],\n",
       "       [0.58907852, 0.41092148],\n",
       "       [0.40210957, 0.59789043],\n",
       "       [0.90327299, 0.09672701],\n",
       "       [0.91263534, 0.08736466],\n",
       "       [0.89873479, 0.10126521],\n",
       "       [0.92617101, 0.07382899],\n",
       "       [0.68966406, 0.31033594],\n",
       "       [0.91257249, 0.08742751],\n",
       "       [0.46062509, 0.53937491],\n",
       "       [0.15303787, 0.84696213],\n",
       "       [0.43482772, 0.56517228],\n",
       "       [0.47391723, 0.52608277],\n",
       "       [0.25748278, 0.74251722],\n",
       "       [0.21655748, 0.78344252],\n",
       "       [0.59974843, 0.40025157],\n",
       "       [0.62001173, 0.37998827],\n",
       "       [0.93889427, 0.06110573],\n",
       "       [0.2292547 , 0.7707453 ],\n",
       "       [0.97426557, 0.02573443],\n",
       "       [0.35093671, 0.64906329],\n",
       "       [0.24244335, 0.75755665],\n",
       "       [0.17739718, 0.82260282],\n",
       "       [0.38023537, 0.61976463],\n",
       "       [0.18903684, 0.81096316],\n",
       "       [0.88043543, 0.11956457],\n",
       "       [0.94586224, 0.05413776],\n",
       "       [0.10128065, 0.89871935],\n",
       "       [0.78571719, 0.21428281],\n",
       "       [0.50005039, 0.49994961],\n",
       "       [0.90267321, 0.09732679],\n",
       "       [0.15695267, 0.84304733],\n",
       "       [0.07522524, 0.92477476],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.5380505 , 0.4619495 ],\n",
       "       [0.87441662, 0.12558338],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.14787048, 0.85212952],\n",
       "       [0.91118137, 0.08881863],\n",
       "       [0.64407995, 0.35592005],\n",
       "       [0.93781983, 0.06218017],\n",
       "       [0.19262121, 0.80737879],\n",
       "       [0.63064721, 0.36935279],\n",
       "       [0.72917221, 0.27082779],\n",
       "       [0.75041859, 0.24958141],\n",
       "       [0.06380334, 0.93619666],\n",
       "       [0.45682672, 0.54317328],\n",
       "       [0.47991914, 0.52008086],\n",
       "       [0.92503499, 0.07496501],\n",
       "       [0.91121644, 0.08878356],\n",
       "       [0.09438578, 0.90561422],\n",
       "       [0.91334816, 0.08665184],\n",
       "       [0.92361443, 0.07638557],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.07596342, 0.92403658],\n",
       "       [0.910487  , 0.089513  ],\n",
       "       [0.8866159 , 0.1133841 ],\n",
       "       [0.909576  , 0.090424  ],\n",
       "       [0.40185246, 0.59814754],\n",
       "       [0.46298676, 0.53701324],\n",
       "       [0.47463573, 0.52536427],\n",
       "       [0.88971298, 0.11028702],\n",
       "       [0.2528464 , 0.7471536 ],\n",
       "       [0.60677327, 0.39322673],\n",
       "       [0.47997678, 0.52002322],\n",
       "       [0.47989578, 0.52010422],\n",
       "       [0.92805899, 0.07194101],\n",
       "       [0.78689644, 0.21310356],\n",
       "       [0.78946182, 0.21053818],\n",
       "       [0.653666  , 0.346334  ],\n",
       "       [0.37377535, 0.62622465],\n",
       "       [0.87823512, 0.12176488],\n",
       "       [0.80191946, 0.19808054],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.72021804, 0.27978196],\n",
       "       [0.09880914, 0.90119086],\n",
       "       [0.74596487, 0.25403513],\n",
       "       [0.88604926, 0.11395074],\n",
       "       [0.79300335, 0.20699665],\n",
       "       [0.93105877, 0.06894123],\n",
       "       [0.23865703, 0.76134297],\n",
       "       [0.53511777, 0.46488223],\n",
       "       [0.1785977 , 0.8214023 ],\n",
       "       [0.48998811, 0.51001189],\n",
       "       [0.87441662, 0.12558338],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.76395002, 0.23604998],\n",
       "       [0.90960517, 0.09039483],\n",
       "       [0.91042517, 0.08957483],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.90571844, 0.09428156],\n",
       "       [0.90447089, 0.09552911],\n",
       "       [0.95293523, 0.04706477],\n",
       "       [0.30618599, 0.69381401],\n",
       "       [0.4929127 , 0.5070873 ],\n",
       "       [0.2961696 , 0.7038304 ],\n",
       "       [0.71439248, 0.28560752],\n",
       "       [0.91255795, 0.08744205],\n",
       "       [0.54433809, 0.45566191],\n",
       "       [0.91828294, 0.08171706],\n",
       "       [0.41044274, 0.58955726],\n",
       "       [0.08645153, 0.91354847],\n",
       "       [0.55471497, 0.44528503],\n",
       "       [0.61734348, 0.38265652],\n",
       "       [0.91547814, 0.08452186],\n",
       "       [0.45588589, 0.54411411],\n",
       "       [0.72120442, 0.27879558],\n",
       "       [0.91113274, 0.08886726],\n",
       "       [0.81652225, 0.18347775],\n",
       "       [0.471268  , 0.528732  ],\n",
       "       [0.2421947 , 0.7578053 ],\n",
       "       [0.91118097, 0.08881903],\n",
       "       [0.54336309, 0.45663691],\n",
       "       [0.78198986, 0.21801014],\n",
       "       [0.91827369, 0.08172631],\n",
       "       [0.08509182, 0.91490818],\n",
       "       [0.11649532, 0.88350468],\n",
       "       [0.90651823, 0.09348177],\n",
       "       [0.47463573, 0.52536427],\n",
       "       [0.89682418, 0.10317582],\n",
       "       [0.91385383, 0.08614617],\n",
       "       [0.91118097, 0.08881903],\n",
       "       [0.94404626, 0.05595374],\n",
       "       [0.47986014, 0.52013986],\n",
       "       [0.90978895, 0.09021105],\n",
       "       [0.85169978, 0.14830022],\n",
       "       [0.87412104, 0.12587896],\n",
       "       [0.11313893, 0.88686107],\n",
       "       [0.87033494, 0.12966506],\n",
       "       [0.04336037, 0.95663963],\n",
       "       [0.89336837, 0.10663163],\n",
       "       [0.7963114 , 0.2036886 ],\n",
       "       [0.09431929, 0.90568071],\n",
       "       [0.4616627 , 0.5383373 ],\n",
       "       [0.75030788, 0.24969212],\n",
       "       [0.90635742, 0.09364258],\n",
       "       [0.20732372, 0.79267628],\n",
       "       [0.81173636, 0.18826364],\n",
       "       [0.2458473 , 0.7541527 ],\n",
       "       [0.47555199, 0.52444801],\n",
       "       [0.47983677, 0.52016323],\n",
       "       [0.1869618 , 0.8130382 ],\n",
       "       [0.87033827, 0.12966173],\n",
       "       [0.9103866 , 0.0896134 ],\n",
       "       [0.72917221, 0.27082779],\n",
       "       [0.89682418, 0.10317582],\n",
       "       [0.67580402, 0.32419598],\n",
       "       [0.76395002, 0.23604998],\n",
       "       [0.83426151, 0.16573849],\n",
       "       [0.21077103, 0.78922897],\n",
       "       [0.91825632, 0.08174368],\n",
       "       [0.63215032, 0.36784968],\n",
       "       [0.71439248, 0.28560752],\n",
       "       [0.26343384, 0.73656616],\n",
       "       [0.86179873, 0.13820127],\n",
       "       [0.34885788, 0.65114212],\n",
       "       [0.16058177, 0.83941823],\n",
       "       [0.51907595, 0.48092405],\n",
       "       [0.87820035, 0.12179965],\n",
       "       [0.4616627 , 0.5383373 ],\n",
       "       [0.91106332, 0.08893668],\n",
       "       [0.63310794, 0.36689206],\n",
       "       [0.618143  , 0.381857  ],\n",
       "       [0.25501886, 0.74498114],\n",
       "       [0.91041111, 0.08958889],\n",
       "       [0.94586981, 0.05413019],\n",
       "       [0.39526673, 0.60473327],\n",
       "       [0.37683754, 0.62316246],\n",
       "       [0.0608691 , 0.9391309 ],\n",
       "       [0.90967358, 0.09032642],\n",
       "       [0.88612631, 0.11387369],\n",
       "       [0.50168858, 0.49831142],\n",
       "       [0.47739722, 0.52260278],\n",
       "       [0.09784798, 0.90215202],\n",
       "       [0.77553339, 0.22446661],\n",
       "       [0.44770924, 0.55229076],\n",
       "       [0.08318544, 0.91681456],\n",
       "       [0.91826741, 0.08173259],\n",
       "       [0.881269  , 0.118731  ],\n",
       "       [0.78273926, 0.21726074],\n",
       "       [0.882261  , 0.117739  ],\n",
       "       [0.30654904, 0.69345096],\n",
       "       [0.97035638, 0.02964362],\n",
       "       [0.3084537 , 0.6915463 ],\n",
       "       [0.49400586, 0.50599414],\n",
       "       [0.91547586, 0.08452414],\n",
       "       [0.87839934, 0.12160066],\n",
       "       [0.13672643, 0.86327357],\n",
       "       [0.49252863, 0.50747137],\n",
       "       [0.17878582, 0.82121418],\n",
       "       [0.86003313, 0.13996687],\n",
       "       [0.89673163, 0.10326837],\n",
       "       [0.53666755, 0.46333245],\n",
       "       [0.92099015, 0.07900985],\n",
       "       [0.89743097, 0.10256903],\n",
       "       [0.91112356, 0.08887644],\n",
       "       [0.24257455, 0.75742545],\n",
       "       [0.05870095, 0.94129905],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.94866826, 0.05133174],\n",
       "       [0.93563652, 0.06436348],\n",
       "       [0.28105834, 0.71894166],\n",
       "       [0.44532117, 0.55467883],\n",
       "       [0.0512178 , 0.9487822 ],\n",
       "       [0.95763767, 0.04236233],\n",
       "       [0.47502383, 0.52497617],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.57557205, 0.42442795],\n",
       "       [0.44104029, 0.55895971],\n",
       "       [0.68407081, 0.31592919],\n",
       "       [0.90336246, 0.09663754],\n",
       "       [0.56153612, 0.43846388],\n",
       "       [0.52594105, 0.47405895],\n",
       "       [0.7147541 , 0.2852459 ],\n",
       "       [0.16058177, 0.83941823],\n",
       "       [0.05236182, 0.94763818],\n",
       "       [0.90954896, 0.09045104],\n",
       "       [0.68407081, 0.31592919],\n",
       "       [0.18005545, 0.81994455],\n",
       "       [0.25043684, 0.74956316],\n",
       "       [0.39414761, 0.60585239],\n",
       "       [0.77052274, 0.22947726],\n",
       "       [0.83661598, 0.16338402],\n",
       "       [0.47974582, 0.52025418],\n",
       "       [0.34264621, 0.65735379],\n",
       "       [0.47439473, 0.52560527],\n",
       "       [0.65561264, 0.34438736],\n",
       "       [0.72184268, 0.27815732],\n",
       "       [0.88591742, 0.11408258],\n",
       "       [0.68388636, 0.31611364],\n",
       "       [0.49054931, 0.50945069],\n",
       "       [0.49801469, 0.50198531],\n",
       "       [0.68343183, 0.31656817],\n",
       "       [0.87026489, 0.12973511],\n",
       "       [0.74346176, 0.25653824],\n",
       "       [0.43862678, 0.56137322],\n",
       "       [0.7898327 , 0.2101673 ],\n",
       "       [0.30063143, 0.69936857],\n",
       "       [0.89675853, 0.10324147],\n",
       "       [0.47986014, 0.52013986],\n",
       "       [0.03972777, 0.96027223],\n",
       "       [0.10390338, 0.89609662],\n",
       "       [0.83426151, 0.16573849],\n",
       "       [0.05629509, 0.94370491],\n",
       "       [0.17336387, 0.82663613],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.9103866 , 0.0896134 ],\n",
       "       [0.38520306, 0.61479694],\n",
       "       [0.88614122, 0.11385878],\n",
       "       [0.65835958, 0.34164042],\n",
       "       [0.53208418, 0.46791582],\n",
       "       [0.8967485 , 0.1032515 ],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.90503604, 0.09496396],\n",
       "       [0.52327162, 0.47672838],\n",
       "       [0.09076989, 0.90923011],\n",
       "       [0.9638702 , 0.0361298 ],\n",
       "       [0.89142756, 0.10857244],\n",
       "       [0.94394627, 0.05605373],\n",
       "       [0.65954579, 0.34045421],\n",
       "       [0.91119811, 0.08880189],\n",
       "       [0.89328437, 0.10671563],\n",
       "       [0.75540583, 0.24459417],\n",
       "       [0.86600266, 0.13399734],\n",
       "       [0.8191536 , 0.1808464 ],\n",
       "       [0.90009063, 0.09990937],\n",
       "       [0.10949808, 0.89050192],\n",
       "       [0.89309219, 0.10690781],\n",
       "       [0.87840828, 0.12159172],\n",
       "       [0.13947239, 0.86052761],\n",
       "       [0.06049854, 0.93950146],\n",
       "       [0.39107981, 0.60892019],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.49119923, 0.50880077],\n",
       "       [0.88604926, 0.11395074],\n",
       "       [0.37611613, 0.62388387],\n",
       "       [0.47986014, 0.52013986],\n",
       "       [0.67541544, 0.32458456],\n",
       "       [0.90007423, 0.09992577],\n",
       "       [0.7551482 , 0.2448518 ],\n",
       "       [0.54426113, 0.45573887],\n",
       "       [0.37881381, 0.62118619],\n",
       "       [0.86619913, 0.13380087],\n",
       "       [0.92617101, 0.07382899],\n",
       "       [0.94537293, 0.05462707],\n",
       "       [0.18826793, 0.81173207],\n",
       "       [0.911173  , 0.088827  ],\n",
       "       [0.76546743, 0.23453257],\n",
       "       [0.55994847, 0.44005153],\n",
       "       [0.47982202, 0.52017798],\n",
       "       [0.0785048 , 0.9214952 ],\n",
       "       [0.22252607, 0.77747393],\n",
       "       [0.10551664, 0.89448336],\n",
       "       [0.49384336, 0.50615664],\n",
       "       [0.30227628, 0.69772372],\n",
       "       [0.15895297, 0.84104703],\n",
       "       [0.79589889, 0.20410111],\n",
       "       [0.47733688, 0.52266312],\n",
       "       [0.67580402, 0.32419598],\n",
       "       [0.79973904, 0.20026096],\n",
       "       [0.78946182, 0.21053818],\n",
       "       [0.76345576, 0.23654424],\n",
       "       [0.70725911, 0.29274089],\n",
       "       [0.79285536, 0.20714464],\n",
       "       [0.47977163, 0.52022837],\n",
       "       [0.04893497, 0.95106503],\n",
       "       [0.89335805, 0.10664195],\n",
       "       [0.07180096, 0.92819904],\n",
       "       [0.63310794, 0.36689206],\n",
       "       [0.05706475, 0.94293525],\n",
       "       [0.77652775, 0.22347225],\n",
       "       [0.09953016, 0.90046984],\n",
       "       [0.35785451, 0.64214549],\n",
       "       [0.78342442, 0.21657558],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.10520695, 0.89479305],\n",
       "       [0.92927653, 0.07072347],\n",
       "       [0.87912816, 0.12087184],\n",
       "       [0.05606215, 0.94393785],\n",
       "       [0.91109644, 0.08890356],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.07361521, 0.92638479],\n",
       "       [0.42368228, 0.57631772],\n",
       "       [0.88603086, 0.11396914],\n",
       "       [0.49354663, 0.50645337],\n",
       "       [0.94199896, 0.05800104],\n",
       "       [0.46406922, 0.53593078],\n",
       "       [0.87543791, 0.12456209],\n",
       "       [0.47463573, 0.52536427],\n",
       "       [0.91826741, 0.08173259],\n",
       "       [0.80426916, 0.19573084],\n",
       "       [0.49349745, 0.50650255],\n",
       "       [0.94194377, 0.05805623],\n",
       "       [0.27125918, 0.72874082],\n",
       "       [0.90503604, 0.09496396],\n",
       "       [0.42139363, 0.57860637],\n",
       "       [0.48632597, 0.51367403],\n",
       "       [0.90995494, 0.09004506],\n",
       "       [0.89325572, 0.10674428],\n",
       "       [0.90503604, 0.09496396],\n",
       "       [0.37597976, 0.62402024],\n",
       "       [0.28980378, 0.71019622],\n",
       "       [0.877409  , 0.122591  ],\n",
       "       [0.76159732, 0.23840268],\n",
       "       [0.91118097, 0.08881903],\n",
       "       [0.91111718, 0.08888282],\n",
       "       [0.54123605, 0.45876395],\n",
       "       [0.90961701, 0.09038299],\n",
       "       [0.47986014, 0.52013986],\n",
       "       [0.10005643, 0.89994357],\n",
       "       [0.86866683, 0.13133317],\n",
       "       [0.91110242, 0.08889758],\n",
       "       [0.20667212, 0.79332788],\n",
       "       [0.9399833 , 0.0600167 ],\n",
       "       [0.8858756 , 0.1141244 ],\n",
       "       [0.37470149, 0.62529851],\n",
       "       [0.91113114, 0.08886886],\n",
       "       [0.42810332, 0.57189668],\n",
       "       [0.65973737, 0.34026263],\n",
       "       [0.9065224 , 0.0934776 ],\n",
       "       [0.55345143, 0.44654857],\n",
       "       [0.26185721, 0.73814279],\n",
       "       [0.9000091 , 0.0999909 ],\n",
       "       [0.68274417, 0.31725583]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba2 = logit2.predict_proba(X_train2)\n",
    "y_pred_proba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.78\n",
      " Baseline Accuracy is 0.6164658634538153\n",
      "The model performed better than the baseline and the 1st model without sex\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit2.score(X_train2, y_train)))\n",
    "print(f' Baseline Accuracy is {baseline_accuracy}')\n",
    "print('The model performed better than the baseline and the 1st model without sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try out other combinations of features and models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = train.drop(columns=['survived', 'sex', 'embark_town'])\n",
    "\n",
    "X_validate3= validate.drop(columns=['survived', 'sex', 'embark_town'])\n",
    "\n",
    "X_test3 = test.drop(columns=['survived', 'sex', 'embark_town'])\n",
    "\n",
    "y_train = train.survived\n",
    "\n",
    "y_validate = validate.survived \n",
    "\n",
    "y_test = test.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit3 = LogisticRegression(C=1, random_state=123)\n",
    "logit3.fit(X_train3, y_train)\n",
    "\n",
    "y_pred3 = logit3.predict(X_train3)\n",
    "y_pred_proba3    = logit3.predict_proba(X_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.80\n",
      " Baseline Accuracy is 0.6164658634538153\n",
      "The model performed better than the baseline and has all features\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit3.score(X_train3, y_train)))\n",
    "print(f' Baseline Accuracy is {baseline_accuracy}')\n",
    "print('The model performed better than the baseline and has all features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use you best 3 models to predict and evaluate on your validate sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_validate = logit.predict(X_validate)\n",
    "y_pred_validate2 = logit2.predict(X_validate2)\n",
    "y_pred_validate3 = logit3.predict(X_validate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.87      0.77       132\n",
      "           1       0.65      0.39      0.49        82\n",
      "\n",
      "    accuracy                           0.69       214\n",
      "   macro avg       0.68      0.63      0.63       214\n",
      "weighted avg       0.68      0.69      0.66       214\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2: solver = lbfgs, c = 1\n",
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       132\n",
      "           1       0.73      0.71      0.72        82\n",
      "\n",
      "    accuracy                           0.79       214\n",
      "   macro avg       0.78      0.77      0.78       214\n",
      "weighted avg       0.79      0.79      0.79       214\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3: solver = lbfgs, c = 1\n",
      "Accuracy: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84       132\n",
      "           1       0.76      0.68      0.72        82\n",
      "\n",
      "    accuracy                           0.79       214\n",
      "   macro avg       0.79      0.77      0.78       214\n",
      "weighted avg       0.79      0.79      0.79       214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_validate, y_validate)))\n",
    "\n",
    "print(classification_report(y_validate, y_pred_validate))\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "print(\"Model 2: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit2.score(X_validate2, y_validate)))\n",
    "\n",
    "print(classification_report(y_validate, y_pred_validate2))\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "print(\"Model 3: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit3.score(X_validate3, y_validate)))\n",
    "\n",
    "print(classification_report(y_validate, y_pred_validate3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Choose you best model from the validation performation, and evaluate it on the test dataset. How do the performance metrics compare to validate? to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test2 = logit2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2: solver = lbfgs, c = 1\n",
      "Accuracy: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       110\n",
      "           1       0.75      0.81      0.78        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.82      0.81       179\n",
      "weighted avg       0.83      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 2: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit2.score(X_test2, y_test)))\n",
    "\n",
    "print(classification_report(y_test, y_pred_test2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
